# -*- coding: utf-8 -*-
"""GES DISC GPM Downloader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HvvcZIz0LyOjQKab5UaIf_hvsvHZQ3N5

# **Libraries**
"""

# !pip install pydap
# !pip install xarray
# !pip install netCDF4
# !pip install netcdf4
# !pip install scipy
# !pip install h5netcdf

import time
import os
import warnings
import requests as req
import xarray as xr
import netCDF4 as nc4
import pandas as pd
from datetime import date
from datetime import datetime
from glob import glob
from google.colab import drive
from tqdm.notebook import tqdm
from requests.auth import HTTPBasicAuth

warnings.filterwarnings("ignore")
session = req.Session()

mount_folder = "/content/drive/"
drive_folder = mount_folder + "MyDrive/"
drive.mount(mount_folder)

program = "GES DISC Downloader/"
root_dir = drive_folder + program

"""# **Functions**"""

def create_folder(dir):
  if not os.path.isdir(dir):
    os.mkdir(dir)
    print("Folder created at", dir)
  else:
    print("Folder already exist at", dir)
  return dir

def create_coordinate_rectangle(lat, lon, range):
  left = round(float(lon) - range, 3)
  bottom = round(float(lat) - range, 3)
  right = round(float(lon) + range, 3)
  top = round(float(lat) + range, 3)
  return [left, bottom, right, top ]

def request_timeseries_list(payload):
  url = "https://disc.gsfc.nasa.gov/service/subset/jsonwsp"
  response = session.post(url, json = payload).json()
  job_id = response["result"]["jobId"]
  session_id = response["result"]["sessionId"]
  return job_id, session_id

def get_result(job_id, session_id):
  url = "https://disc.gsfc.nasa.gov/service/subset/jsonwsp"
  payload = {"methodname":"GetStatus","args":{"jobId":job_id,"sessionId":session_id},"type":"jsonwsp/request","version":"1.0"}
  percent = 0
  while percent < 100:
    os.system("cls")
    time.sleep(7)
    response = session.post(url, json = payload).json()
    percent = response["result"]["PercentCompleted"]
    text = response["result"]["Status"]
    print(text, percent, "%")
  print("https://disc.gsfc.nasa.gov/api/jobs/results/" + job_id)
  return session.get("https://disc.gsfc.nasa.gov/api/jobs/results/" + job_id)

def create_payload(coordinates, data, start, end):
  start = date.fromisoformat(start).strftime("%Y-%m-%dT00:00:00.000Z")
  end = date.fromisoformat(end).strftime("%Y-%m-%dT23:59:59.999Z")
  return {"methodname": "subset","args": {"start": start,"end": end,"box": coordinates,"crop": True,"format": "netCDF","agent": "OPeNDAP","role": "subset","data": data},"type": "jsonwsp/request","version": "1.0"}

def save_file(file_name, data):
  file = open(file_name, "w")
  file.write(data)
  file.close()
  print("Saved to", file_name)
  return file_name

def get_all_url(text):
  open_file = open(text, "r")
  list_url = open_file.read().splitlines()[2:]
  return list_url

def get_doc_url(text):
  open_file = open(text, "r")
  list_url = open_file.read().splitlines()[:2]
  return list_url

def download_file(url, auth):
  response = session.get(url, stream = True)
  if response.url:
    return session.get(response.url, auth = HTTPBasicAuth(auth[0], auth[1]))
  return response

def get_nasa_product_id(url):
  return url.split("/")[-2]

def create_variable(product, variable):
  return {"datasetId": product, "variable": variable}

def create_variables(product, variables):
  temp = []
  for variable in variables:
    temp.append(create_variable(product, variable))
  return temp

def create_dataframes(file_list, folder):
  return pd.concat([xr.open_dataset(folder + nc4_file).to_dataframe() for nc4_file in file_list])

"""# **Initialization (Change here)**

### **Folder**
"""

create_folder(root_dir)

today_folder = root_dir + str(date.today()) + "/"
create_folder(today_folder)

"""### **Product URL (Change here)**"""

product_url = "https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGDF_06/summary"

"""### **Coordinate (Change here)**"""

lat = "-3.44200"
lon = "114.75400"
range = 0.02

# box_coordinate = create_coordinate_rectangle(lat, lon, range)
# box_coordinate = [95,-11,141,6]
# box_coordinate = [114,-5.20,117.40,-1.10]
box_coordinate = [105.1, -8.87, 114.5, -5.8]

"""### **Time (Change here)**"""

start = "2000-06-01"
end = "2021-10-01"

"""### **Variables (Change here)**"""

variables = ["precipitationCal"]
# variables = ["HQprecipitation", "HQprecipitation_cnt", "HQprecipitation_cnt_cond", "precipitationCal", "precipitationCal_cnt", "precipitationCal_cnt_cond", "randomError", "randomError_cnt", "time_bnds"]
# variables = ["HQobservationTime", "HQprecipitation", "HQprecipSource", "IRkalmanFilterWeight", "IRprecipitation", "lat_bnds", "lon_bnds", "precipitationCal", "precipitationQualityIndex", "precipitationUncal", "probabilityLiquidPrecipitation", "randomError", "time_bnds"]

"""### **URL List File Name**"""

list_name_txt = "GPM URL List to download.txt"

"""### **Autorization (Change here)**"""

username = "zekhoi"
password = "Manakali32!"

"""# **Run**

## **Generate**

### **Generate Folder**
"""

product = get_nasa_product_id(product_url)
product_folder = today_folder + product + " from " + start + " to " + end + " at " + str(datetime.now().strftime("%H:%M")) + ""+ "/" 
create_folder(product_folder)

result_raw_folder = product_folder + "raw/"
create_folder(result_raw_folder)

result_merged_folder = product_folder + "merged/"
create_folder(result_merged_folder)

"""### **Generate Data for Payload**"""

data = create_variables(product, variables)

"""### **Generate Payload**"""

payload = create_payload(box_coordinate, data, start, end)

"""### **Authorization**"""

authorization = (username, password)

"""## **Download**"""

job_id, session_id = request_timeseries_list(payload)

list_download = get_result(job_id, session_id)

urls_path = save_file(product_folder + list_name_txt, list_download.text)

urls = get_all_url(urls_path)

docs = get_doc_url(urls_path)

nc4_file_list = []

for doc in docs:
  doc_name = doc.split("/")[-1]
  f = open(product_folder + doc_name, "wb")
  content = download_file(doc, authorization).content
  f.write(content)
  f.close()

for url in tqdm(urls):
# for url in urls:
  name = url.split("/")[-1].split(".")[4] + ".nc4"
  f = open(result_raw_folder + name, "wb")
  content = download_file(url, authorization).content
  f.write(content)
  f.close()
  nc4_file_list.append(name)

"""## **Merge Dataframe**"""

dataframe = create_dataframes(nc4_file_list, result_raw_folder)

"""## **Save Merged Dataframe**

### **File Name**
"""

merged_file_name = result_merged_folder + start + " to " + end + ".csv"

"""### **Save**"""

dataframe.to_csv(merged_file_name)

"""# **Preview**"""

# pd.read_csv(merged_file_name).head()